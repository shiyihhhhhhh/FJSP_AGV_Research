"""
è‡ªé€‚åº”NSGA-IIç®—æ³• - é›†æˆQ-Learningçš„è‡ªé€‚åº”å¤šç›®æ ‡ä¼˜åŒ–
"""

import numpy as np
from typing import List, Dict, Tuple, Any
import random
from .nsga2_algorithm import NSGA2
from ..reinforcement_learning.q_learning import QLearning

class AdaptiveNSGA2(NSGA2):
    """è‡ªé€‚åº”NSGA-IIç®—æ³• - é›†æˆQ-Learningçš„è‡ªé€‚åº”ä¼˜åŒ–"""

    def __init__(self, problem_data: Dict[str, Any], population_size: int = 100,
                 crossover_rate: float = 0.8, mutation_rate: float = 0.1,
                 max_generations: int = 100, q_learning_config: Dict = None):

        super().__init__(problem_data, population_size, crossover_rate, mutation_rate, max_generations)

        # Q-Learningé…ç½®
        if q_learning_config is None:
            q_learning_config = {
                'learning_rate': 0.1,
                'discount_factor': 0.9,
                'exploration_rate': 0.2
            }

        # å®šä¹‰çŠ¶æ€ç©ºé—´
        self.state_space = self._define_state_space()

        # å®šä¹‰åŠ¨ä½œç©ºé—´ï¼ˆä¸åŒçš„æœç´¢ç­–ç•¥ï¼‰
        self.action_space = self._define_action_space()

        # åˆå§‹åŒ–Q-Learning
        self.q_learning = QLearning(
            state_space=self.state_space,
            action_space=self.action_space,
            learning_rate=q_learning_config['learning_rate'],
            discount_factor=q_learning_config['discount_factor'],
            exploration_rate=q_learning_config['exploration_rate']
        )

        # è‡ªé€‚åº”å‚æ•°
        self.adaptive_parameters = {
            'crossover_rate': crossover_rate,
            'mutation_rate': mutation_rate,
            'selection_pressure': 2.0
        }

        # å­¦ä¹ å†å²
        self.learning_history = {
            'states': [],
            'actions': [],
            'rewards': [],
            'q_values': []
        }

        # æ€§èƒ½è·Ÿè¸ª
        self.performance_tracking = {
            'population_diversity': [],
            'improvement_trend': [],
            'search_stage': []
        }

    def _define_state_space(self) -> List[str]:
        """å®šä¹‰çŠ¶æ€ç©ºé—´"""
        states = []

        # ç§ç¾¤å¤šæ ·æ€§çŠ¶æ€
        diversity_states = ['low_diversity', 'medium_diversity', 'high_diversity']

        # æ”¹è¿›è¶‹åŠ¿çŠ¶æ€
        improvement_states = ['improving', 'stable', 'deteriorating']

        # æœç´¢é˜¶æ®µçŠ¶æ€
        stage_states = ['early_stage', 'middle_stage', 'late_stage']

        # å¸•ç´¯æ‰˜å‰æ²¿å¤§å°çŠ¶æ€
        front_states = ['small_front', 'medium_front', 'large_front']

        # ç»„åˆçŠ¶æ€
        for div in diversity_states:
            for imp in improvement_states:
                for stage in stage_states:
                    for front in front_states:
                        states.append(f"{div}_{imp}_{stage}_{front}")

        return states

    def _define_action_space(self) -> List[str]:
        """å®šä¹‰åŠ¨ä½œç©ºé—´ï¼ˆä¸åŒçš„æœç´¢ç­–ç•¥ï¼‰"""
        actions = [
            # åŸºç¡€é—ä¼ æ“ä½œç»„åˆ
            'standard_pox_uniform',      # æ ‡å‡†POXäº¤å‰ + å‡åŒ€å˜å¼‚
            'enhanced_pox_swap',         # å¢å¼ºPOXäº¤å‰ + äº¤æ¢å˜å¼‚
            'multi_point_crossover',     # å¤šç‚¹äº¤å‰
            'intensive_mutation',        # å¼ºåŒ–å˜å¼‚

            # è‡ªé€‚åº”å‚æ•°è°ƒæ•´
            'increase_crossover',        # å¢åŠ äº¤å‰ç‡
            'increase_mutation',         # å¢åŠ å˜å¼‚ç‡
            'decrease_crossover',        # å‡å°‘äº¤å‰ç‡
            'decrease_mutation',         # å‡å°‘å˜å¼‚ç‡

            # ç‰¹æ®Šæœç´¢ç­–ç•¥
            'diversity_preservation',    # å¤šæ ·æ€§ä¿æŒ
            'convergence_acceleration',  # æ”¶æ•›åŠ é€Ÿ
            'local_refinement',          # å±€éƒ¨ç²¾ç»†åŒ–æœç´¢
            'global_exploration'         # å…¨å±€æ¢ç´¢
        ]
        return actions

    def _calculate_population_diversity(self) -> float:
        """è®¡ç®—ç§ç¾¤å¤šæ ·æ€§"""
        if len(self.population) <= 1:
            return 0.0

        # åŸºäºç›®æ ‡ç©ºé—´çš„å¤šæ ·æ€§
        objectives = np.array([chrom.objectives for chrom in self.population])

        # å½’ä¸€åŒ–ç›®æ ‡å€¼
        normalized_objs = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-10)

        # è®¡ç®—å¹³å‡æ¬§æ°è·ç¦»
        total_distance = 0
        count = 0
        for i in range(len(normalized_objs)):
            for j in range(i + 1, len(normalized_objs)):
                distance = np.linalg.norm(normalized_objs[i] - normalized_objs[j])
                total_distance += distance
                count += 1

        return total_distance / count if count > 0 else 0.0

    def _assess_improvement_trend(self) -> str:
        """è¯„ä¼°æ”¹è¿›è¶‹åŠ¿"""
        if len(self.history['best_fitness']) < 5:
            return 'stable'

        recent_improvements = []
        for i in range(1, min(5, len(self.history['best_fitness']))):
            improvement = self.history['best_fitness'][i-1] - self.history['best_fitness'][i]
            recent_improvements.append(improvement)

        avg_improvement = np.mean(recent_improvements)

        if avg_improvement > 0.01:
            return 'improving'
        elif avg_improvement < -0.01:
            return 'deteriorating'
        else:
            return 'stable'

    def _determine_search_stage(self, current_gen: int) -> str:
        """ç¡®å®šæœç´¢é˜¶æ®µ"""
        progress = current_gen / self.max_generations

        if progress < 0.3:
            return 'early_stage'
        elif progress < 0.7:
            return 'middle_stage'
        else:
            return 'late_stage'

    def _get_current_state(self, current_gen: int) -> str:
        """è·å–å½“å‰çŠ¶æ€"""
        # è®¡ç®—ç§ç¾¤å¤šæ ·æ€§
        diversity = self._calculate_population_diversity()
        if diversity < 0.1:
            diversity_state = 'low_diversity'
        elif diversity < 0.3:
            diversity_state = 'medium_diversity'
        else:
            diversity_state = 'high_diversity'

        # è¯„ä¼°æ”¹è¿›è¶‹åŠ¿
        improvement_state = self._assess_improvement_trend()

        # ç¡®å®šæœç´¢é˜¶æ®µ
        stage_state = self._determine_search_stage(current_gen)

        # å¸•ç´¯æ‰˜å‰æ²¿å¤§å°
        front_size = len(self.pareto_front) if self.pareto_front else 0
        if front_size < 5:
            front_state = 'small_front'
        elif front_size < 15:
            front_state = 'medium_front'
        else:
            front_state = 'large_front'

        # ç»„åˆçŠ¶æ€
        current_state = f"{diversity_state}_{improvement_state}_{stage_state}_{front_state}"

        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        self.performance_tracking['population_diversity'].append(diversity)
        self.performance_tracking['improvement_trend'].append(improvement_state)
        self.performance_tracking['search_stage'].append(stage_state)

        return current_state

    def _calculate_reward(self, old_pareto_front: List, new_pareto_front: List) -> float:
        """è®¡ç®—å¥–åŠ±å€¼"""
        if not old_pareto_front or not new_pareto_front:
            return 0.0

        # è®¡ç®—è¶…ä½“ç§¯æ”¹è¿›
        old_hv = self._calculate_hypervolume(old_pareto_front)
        new_hv = self._calculate_hypervolume(new_pareto_front)
        hv_improvement = new_hv - old_hv

        # è®¡ç®—å¸•ç´¯æ‰˜å‰æ²¿å¤§å°æ”¹è¿›
        old_size = len(old_pareto_front)
        new_size = len(new_pareto_front)
        size_improvement = (new_size - old_size) / max(old_size, 1)

        # è®¡ç®—å¤šæ ·æ€§æ”¹è¿›
        old_diversity = self._calculate_population_diversity_for_front(old_pareto_front)
        new_diversity = self._calculate_population_diversity_for_front(new_pareto_front)
        diversity_improvement = new_diversity - old_diversity

        # ç»¼åˆå¥–åŠ±
        reward = (
            0.5 * hv_improvement +      # è¶…ä½“ç§¯æ”¹è¿›æƒé‡
            0.3 * size_improvement +    # å‰æ²¿å¤§å°æ”¹è¿›æƒé‡
            0.2 * diversity_improvement # å¤šæ ·æ€§æ”¹è¿›æƒé‡
        )

        # å½’ä¸€åŒ–å¥–åŠ±
        normalized_reward = max(-1.0, min(1.0, reward * 10))

        return normalized_reward

    def _calculate_population_diversity_for_front(self, front: List) -> float:
        """è®¡ç®—å‰æ²¿å¤šæ ·æ€§"""
        if len(front) <= 1:
            return 0.0

        objectives = np.array([chrom.objectives for chrom in front])
        normalized_objs = (objectives - objectives.min(axis=0)) / (objectives.max(axis=0) - objectives.min(axis=0) + 1e-10)

        total_distance = 0
        count = 0
        for i in range(len(normalized_objs)):
            for j in range(i + 1, len(normalized_objs)):
                distance = np.linalg.norm(normalized_objs[i] - normalized_objs[j])
                total_distance += distance
                count += 1

        return total_distance / count if count > 0 else 0.0

    def _apply_action(self, action: str):
        """åº”ç”¨é€‰æ‹©çš„åŠ¨ä½œ"""
        # æ ¹æ®åŠ¨ä½œè°ƒæ•´ç®—æ³•å‚æ•°æˆ–é€‰æ‹©æ“ä½œç¬¦
        if action == 'increase_crossover':
            self.adaptive_parameters['crossover_rate'] = min(0.95, self.adaptive_parameters['crossover_rate'] + 0.1)
        elif action == 'decrease_crossover':
            self.adaptive_parameters['crossover_rate'] = max(0.1, self.adaptive_parameters['crossover_rate'] - 0.1)
        elif action == 'increase_mutation':
            self.adaptive_parameters['mutation_rate'] = min(0.5, self.adaptive_parameters['mutation_rate'] + 0.05)
        elif action == 'decrease_mutation':
            self.adaptive_parameters['mutation_rate'] = max(0.01, self.adaptive_parameters['mutation_rate'] - 0.05)
        elif action == 'diversity_preservation':
            # å¤šæ ·æ€§ä¿æŒç­–ç•¥
            self.adaptive_parameters['crossover_rate'] = 0.7
            self.adaptive_parameters['mutation_rate'] = 0.2
        elif action == 'convergence_acceleration':
            # æ”¶æ•›åŠ é€Ÿç­–ç•¥
            self.adaptive_parameters['crossover_rate'] = 0.9
            self.adaptive_parameters['mutation_rate'] = 0.05

        # æ›´æ–°ç®—æ³•å‚æ•°
        self.crossover_rate = self.adaptive_parameters['crossover_rate']
        self.mutation_rate = self.adaptive_parameters['mutation_rate']

    def evolve(self):
        """è‡ªé€‚åº”è¿›åŒ–ä¸€ä»£"""
        # è®°å½•å½“å‰å¸•ç´¯æ‰˜å‰æ²¿
        old_pareto_front = self.pareto_front.copy() if self.pareto_front else []

        # è·å–å½“å‰çŠ¶æ€
        current_gen = len(self.history['best_fitness'])
        current_state = self._get_current_state(current_gen)

        # Q-Learningé€‰æ‹©åŠ¨ä½œ
        action = self.q_learning.choose_action(current_state)

        # åº”ç”¨é€‰æ‹©çš„åŠ¨ä½œ
        self._apply_action(action)

        # æ‰§è¡Œæ ‡å‡†çš„è¿›åŒ–è¿‡ç¨‹
        super().evolve()

        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(old_pareto_front, self.pareto_front)

        # è·å–æ–°çŠ¶æ€
        new_state = self._get_current_state(current_gen + 1)

        # æ›´æ–°Q-Learning
        self.q_learning.update(current_state, action, reward, new_state)

        # è®°å½•å­¦ä¹ å†å²
        self.learning_history['states'].append(current_state)
        self.learning_history['actions'].append(action)
        self.learning_history['rewards'].append(reward)
        self.learning_history['q_values'].append(
            self.q_learning.get_q_value(current_state, action)
        )

        # é€æ¸é™ä½æ¢ç´¢ç‡
        if current_gen % 10 == 0:
            self.q_learning.decrease_exploration_rate()

    def run(self, max_generations: int = None):
        """è¿è¡Œè‡ªé€‚åº”NSGA-IIç®—æ³•"""
        if max_generations is None:
            max_generations = self.max_generations

        print(f"ğŸš€ å¼€å§‹è‡ªé€‚åº”NSGA-IIä¼˜åŒ–ï¼Œæœ€å¤§ä»£æ•°: {max_generations}")
        print(f"ğŸ¤– Q-Learningé…ç½®: å­¦ä¹ ç‡={self.q_learning.learning_rate}, "
              f"æŠ˜æ‰£å› å­={self.q_learning.discount_factor}, æ¢ç´¢ç‡={self.q_learning.exploration_rate}")

        # åˆå§‹åŒ–ç§ç¾¤
        self.initialize_population()

        # åˆå§‹éæ”¯é…æ’åº
        fronts = self.fast_non_dominated_sort(self.population)
        for i, front in enumerate(fronts):
            for chrom in front:
                chrom.rank = i

        # è®¡ç®—åˆå§‹æ‹¥æŒ¤åº¦
        for front in fronts:
            self.crowding_distance_assignment(front)

        self._update_pareto_front()

        # è‡ªé€‚åº”è¿›åŒ–å¾ªç¯
        for gen in range(max_generations):
            try:
                self.evolve()

                if (gen + 1) % 10 == 0:
                    best_makespan = min(chrom.objectives[0] for chrom in self.pareto_front) if self.pareto_front else float('inf')
                    stats = self.q_learning.get_learning_statistics()
                    print(f"  ç¬¬ {gen + 1} ä»£ | å¸•ç´¯æ‰˜è§£: {len(self.pareto_front)} | "
                          f"æœ€ä½³å®Œå·¥æ—¶é—´: {best_makespan:.2f} | "
                          f"å¹³å‡å¥–åŠ±: {stats['average_reward']:.3f} | "
                          f"æ¢ç´¢ç‡: {self.q_learning.exploration_rate:.3f}")
            except Exception as e:
                print(f"âš ï¸ ç¬¬ {gen + 1} ä»£è¿›åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")

        print(f"âœ… è‡ªé€‚åº”NSGA-IIä¼˜åŒ–å®Œæˆï¼Œæ‰¾åˆ° {len(self.pareto_front)} ä¸ªå¸•ç´¯æ‰˜æœ€ä¼˜è§£")

        # è¾“å‡ºå­¦ä¹ ç»Ÿè®¡
        self._print_learning_statistics()

        return self.pareto_front

    def _print_learning_statistics(self):
        """è¾“å‡ºå­¦ä¹ ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.q_learning.get_learning_statistics()
        print("\nğŸ“Š Q-Learningå­¦ä¹ ç»Ÿè®¡:")
        print(f"  æ€»æ›´æ–°æ¬¡æ•°: {stats['total_updates']}")
        print(f"  å¹³å‡å¥–åŠ±: {stats['average_reward']:.3f}")
        print(f"  æœ€å¤§å¥–åŠ±: {stats['max_reward']:.3f}")
        print(f"  æœ€å°å¥–åŠ±: {stats['min_reward']:.3f}")
        print(f"  è®¿é—®çŠ¶æ€æ•°: {stats['states_visited']}")
        print(f"  ä½¿ç”¨åŠ¨ä½œæ•°: {stats['unique_actions']}")
        print(f"  æœ€ç»ˆæ¢ç´¢ç‡: {self.q_learning.exploration_rate:.3f}")

        # è¾“å‡ºæœ€å¸¸ç”¨çš„åŠ¨ä½œ
        if self.learning_history['actions']:
            from collections import Counter
            action_counts = Counter(self.learning_history['actions'])
            print(f"  æœ€å¸¸ç”¨åŠ¨ä½œ: {action_counts.most_common(3)}")

    def get_learning_analysis(self) -> Dict[str, Any]:
        """è·å–å­¦ä¹ åˆ†æ"""
        stats = self.q_learning.get_learning_statistics()

        return {
            'q_learning_stats': stats,
            'performance_tracking': self.performance_tracking,
            'learning_history_summary': {
                'total_states': len(set(self.learning_history['states'])),
                'total_actions': len(set(self.learning_history['actions'])),
                'average_reward': np.mean(self.learning_history['rewards']) if self.learning_history['rewards'] else 0,
                'final_q_values': self.learning_history['q_values'][-10:] if len(self.learning_history['q_values']) >= 10 else self.learning_history['q_values']
            }
        }