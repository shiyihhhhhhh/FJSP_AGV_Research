from typing import Dict, Any, List
from .base_algorithm import BaseAlgorithm
from .metaheuristic.nsga2 import NSGA2
from .reinforcement_learning.q_learning import QLearning


class HybridAlgorithm(BaseAlgorithm):
    """å…ƒå¯å‘å¼ + Q-Learningæ··åˆç®—æ³•"""

    def __init__(self, model, config: Dict):
        super().__init__(model, config)
        self.metaheuristic_config = config.get('metaheuristic', {})
        self.rl_config = config.get('reinforcement_learning', {})

        self.metaheuristic = NSGA2(self.metaheuristic_config)
        self.rl_agent = QLearning(self.rl_config)

        # å®šä¹‰RLçŠ¶æ€å’ŒåŠ¨ä½œ
        self.rl_states = ['early_search', 'mid_search', 'late_search']
        self.rl_actions = ['intensify', 'diversify', 'balance']

    def initialize(self):
        """åˆå§‹åŒ–ç®—æ³•"""
        self.metaheuristic.initialize_population()
        print("âœ… æ··åˆç®—æ³•åˆå§‹åŒ–å®Œæˆ")

    def run(self):
        """è¿è¡Œæ··åˆç®—æ³•"""
        print("ğŸš€ å¼€å§‹è¿è¡Œæ··åˆç®—æ³•...")

        for generation in range(self.config.get('max_generations', 100)):
            # å…ƒå¯å‘å¼æœç´¢
            population = self.metaheuristic.population
            fitness = self.metaheuristic.evaluate_population(population)

            # Q-Learningè‡ªé€‚åº”è°ƒæ•´
            state = self._get_search_state(population, fitness, generation)
            action = self.rl_agent.select_action(state)
            self._apply_action(action, population)

            # æ‰§è¡Œå…ƒå¯å‘å¼æ“ä½œ
            selected = self.metaheuristic.selection(population, fitness)
            offspring = self.metaheuristic.crossover(selected)
            offspring = self.metaheuristic.mutation(offspring)

            # æ›´æ–°ç§ç¾¤
            self.metaheuristic.population = self.metaheuristic._create_new_population(selected, offspring)

            # è®¡ç®—å¥–åŠ±å¹¶æ›´æ–°Qå€¼
            reward = self._calculate_reward(population, self.metaheuristic.population)
            next_state = self._get_search_state(self.metaheuristic.population,
                                                self.metaheuristic.evaluate_population(self.metaheuristic.population),
                                                generation + 1)
            self.rl_agent.update(state, action, reward, next_state)

            # è®°å½•æ”¶æ•›æ•°æ®
            best_fitness = min([sum(f.values()) for f in fitness])
            self.convergence_data.append(best_fitness)

            if generation % 10 == 0:
                print(f"ğŸ“Š ç¬¬{generation}ä»£, æœ€ä½³é€‚åº”åº¦: {best_fitness:.4f}")

        # è·å–æœ€ç»ˆè§£
        self.solution = self._extract_solution()
        print("âœ… æ··åˆç®—æ³•è¿è¡Œå®Œæˆ")

    def _get_search_state(self, population: List, fitness: List, generation: int) -> str:
        """è·å–æœç´¢çŠ¶æ€"""
        # åŸºäºç§ç¾¤å¤šæ ·æ€§å’Œæœç´¢è¿›åº¦å®šä¹‰çŠ¶æ€
        diversity = self._calculate_diversity(population)
        progress = generation / self.config.get('max_generations', 100)

        if progress < 0.33:
            return 'early_search'
        elif progress < 0.66:
            return 'mid_search'
        else:
            return 'late_search'

    def _calculate_diversity(self, population: List) -> float:
        """è®¡ç®—ç§ç¾¤å¤šæ ·æ€§"""
        if len(population) <= 1:
            return 0.0

        # ç®€åŒ–å®ç°ï¼šè®¡ç®—è§£ä¹‹é—´çš„å¹³å‡è·ç¦»
        total_distance = 0
        count = 0

        for i in range(len(population)):
            for j in range(i + 1, len(population)):
                # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“è§£çš„ç»“æ„è®¡ç®—è·ç¦»
                total_distance += 1.0  # ç®€åŒ–
                count += 1

        return total_distance / count if count > 0 else 0.0

    def _apply_action(self, action: str, population: List):
        """åº”ç”¨RLé€‰æ‹©çš„åŠ¨ä½œ"""
        if action == 'intensify':
            # å¼ºåŒ–å±€éƒ¨æœç´¢
            self.metaheuristic.mutation_rate *= 0.8
        elif action == 'diversify':
            # å¢åŠ å¤šæ ·æ€§
            self.metaheuristic.mutation_rate *= 1.2
        elif action == 'balance':
            # ä¿æŒå¹³è¡¡
            pass

        # ç¡®ä¿å‚æ•°åœ¨åˆç†èŒƒå›´å†…
        self.metaheuristic.mutation_rate = max(0.01, min(0.5, self.metaheuristic.mutation_rate))

    def _calculate_reward(self, old_population: List, new_population: List) -> float:
        """è®¡ç®—å¥–åŠ±"""
        old_fitness = [sum(f.values()) for f in self.metaheuristic.evaluate_population(old_population)]
        new_fitness = [sum(f.values()) for f in self.metaheuristic.evaluate_population(new_population)]

        old_best = min(old_fitness)
        new_best = min(new_fitness)

        # å¥–åŠ±åŸºäºæ”¹è¿›ç¨‹åº¦
        improvement = old_best - new_best  # è´Ÿå€¼è¡¨ç¤ºæ”¹è¿›
        return improvement * 10  # ç¼©æ”¾å¥–åŠ±

    def _extract_solution(self) -> Dict:
        """ä»æœ€ç»ˆç§ç¾¤ä¸­æå–æœ€ä½³è§£"""
        fitness = self.metaheuristic.evaluate_population(self.metaheuristic.population)
        best_idx = self.metaheuristic._find_best_solution(fitness)
        return self.metaheuristic.population[best_idx]

    def get_solution(self) -> Dict:
        """è·å–è§£"""
        return self.solution