#!/usr/bin/env python3
"""
FJSP-AGVé›†æˆè°ƒåº¦ç ”ç©¶ä¸»ç¨‹åº
"""

import sys
import os
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

try:
    from src.models.fjsp_agv_model import FJSPAGVModel
    from src.models.model_validator import ModelValidator
    from src.utils.data_generator import DataGenerator
    from src.config.model_config import ModelConfig
    from src.config.algorithm_config import AlgorithmConfig
    from src.experiments.model_validation import ModelValidationExperiment
    from src.experiments.algorithm_comparison import AlgorithmComparisonExperiment
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("ğŸ“ è¯·ç¡®ä¿é¡¹ç›®ç»“æ„æ­£ç¡®ï¼Œsrcç›®å½•å­˜åœ¨")
    sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='FJSP-AGVé›†æˆè°ƒåº¦ç ”ç©¶')
    parser.add_argument('--mode', type=str, choices=['model', 'algorithm', 'experiment'],
                        default='model', help='è¿è¡Œæ¨¡å¼: model(æ¨¡å‹éªŒè¯), algorithm(ç®—æ³•æµ‹è¯•), experiment(å®Œæ•´å®éªŒ)')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    if args.mode == 'model':
        run_model_validation()
    elif args.mode == 'algorithm':
        run_algorithm_test()
    elif args.mode == 'experiment':
        run_full_experiment()
    else:
        print("æœªçŸ¥æ¨¡å¼ï¼Œä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©")


def run_model_validation():
    """è¿è¡Œæ¨¡å‹éªŒè¯"""
    print("ğŸ”¬ è¿è¡Œæ¨¡å‹éªŒè¯...")

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs('results', exist_ok=True)

    # åˆ›å»ºé…ç½®
    config = ModelConfig(n_jobs=3, n_machines=2, n_agvs=1)

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_data = DataGenerator.generate_from_config(config)

    # åˆ›å»ºå¹¶æ±‚è§£æ¨¡å‹
    model = FJSPAGVModel(test_data)
    model.create_variables()
    model.set_objective()
    model.add_constraints()

    if model.solve():
        print("âœ… æ¨¡å‹æ±‚è§£æˆåŠŸï¼")

        # éªŒè¯è§£
        validator = ModelValidator(model)
        validation_results = validator.validate_all()
        print(validator.generate_validation_report())

        # ç»˜åˆ¶è°ƒåº¦å›¾
        model.plot_schedule('results/schedule_gantt.png')

        # ç»˜åˆ¶ç”µé‡ä½¿ç”¨å›¾
        validator.plot_battery_usage('results/battery_usage.png')

        # ä¿å­˜æ¨¡å‹ç»“æœ
        model.save_model('results/model_solution.json')

    else:
        print("âŒ æ¨¡å‹æ±‚è§£å¤±è´¥")


def run_algorithm_test():
    """è¿è¡Œç®—æ³•æµ‹è¯•"""
    print("ğŸ”§ è¿è¡Œç®—æ³•æµ‹è¯•...")

    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = ModelConfig(n_jobs=3, n_machines=2, n_agvs=1)

    # åˆ›å»ºç®—æ³•é…ç½®
    algorithm_configs = {
        'NSGA2': {
            'population_size': 50,
            'max_generations': 100,
            'crossover_rate': 0.8,
            'mutation_rate': 0.2
        },
        'Hybrid': {
            'max_generations': 100,
            'metaheuristic': {
                'population_size': 50,
                'crossover_rate': 0.8,
                'mutation_rate': 0.2
            },
            'reinforcement_learning': {
                'learning_rate': 0.1,
                'discount_factor': 0.9,
                'epsilon': 0.1,
                'actions': ['intensify', 'diversify', 'balance']
            }
        }
    }

    # è¿è¡Œç®—æ³•æ¯”è¾ƒ
    experiment = AlgorithmComparisonExperiment()
    results = experiment.run_comparison(model_config, algorithm_configs)

    # ç”ŸæˆæŠ¥å‘Š
    print(experiment.generate_comparison_report())

    # ç»˜åˆ¶æ”¶æ•›æ›²çº¿
    experiment.plot_convergence_comparison('results/convergence_comparison.png')


def run_full_experiment():
    """è¿è¡Œå®Œæ•´å®éªŒ"""
    print("ğŸ¯ è¿è¡Œå®Œæ•´å®éªŒ...")

    # 1. æ¨¡å‹éªŒè¯å®éªŒ
    print("\n" + "=" * 50)
    print("é˜¶æ®µ1: æ¨¡å‹éªŒè¯å®éªŒ")
    print("=" * 50)

    model_experiment = ModelValidationExperiment()

    # å®šä¹‰å¤šä¸ªæµ‹è¯•å®ä¾‹
    test_configs = [
        ModelConfig(n_jobs=2, n_machines=2, n_agvs=1, operations_per_job=[2, 2]),
        ModelConfig(n_jobs=3, n_machines=2, n_agvs=1, operations_per_job=[2, 2, 2]),
        ModelConfig(n_jobs=4, n_machines=3, n_agvs=1, operations_per_job=[2, 2, 2, 2]),
    ]

    model_results = model_experiment.run_multiple_instances(test_configs)
    print(model_experiment.generate_report())
    model_experiment.plot_performance_comparison('results/model_performance_comparison.png')

    # 2. ç®—æ³•æ¯”è¾ƒå®éªŒ
    print("\n" + "=" * 50)
    print("é˜¶æ®µ2: ç®—æ³•æ¯”è¾ƒå®éªŒ")
    print("=" * 50)

    algorithm_experiment = AlgorithmComparisonExperiment()

    # ä½¿ç”¨ä¸­ç­‰è§„æ¨¡çš„å®ä¾‹
    algo_model_config = ModelConfig(n_jobs=5, n_machines=3, n_agvs=2, operations_per_job=[3, 3, 3, 3, 3])

    algorithm_configs = {
        'NSGA2': {
            'population_size': 100,
            'max_generations': 200,
            'crossover_rate': 0.8,
            'mutation_rate': 0.2
        },
        'Hybrid': {
            'max_generations': 200,
            'metaheuristic': {
                'population_size': 100,
                'crossover_rate': 0.8,
                'mutation_rate': 0.2
            },
            'reinforcement_learning': {
                'learning_rate': 0.1,
                'discount_factor': 0.9,
                'epsilon': 0.1,
                'actions': ['intensify', 'diversify', 'balance']
            }
        }
    }

    algo_results = algorithm_experiment.run_comparison(algo_model_config, algorithm_configs)
    print(algorithm_experiment.generate_comparison_report())
    algorithm_experiment.plot_convergence_comparison('results/algorithm_convergence_comparison.png')

    print("\nğŸ‰ å®Œæ•´å®éªŒå®Œæˆï¼")
    print("ğŸ“ ç»“æœå·²ä¿å­˜è‡³ results/ ç›®å½•")


if __name__ == "__main__":
    main()

